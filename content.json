{"meta":{"title":"Jetinzhang Blog","subtitle":null,"description":null,"author":"Jetinzhang","url":"https://jetinzhang.github.io","root":"/"},"pages":[{"title":"关于","date":"2019-10-25T06:01:03.000Z","updated":"2019-10-29T14:18:28.623Z","comments":true,"path":"about/index.html","permalink":"https://jetinzhang.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-10-25T05:46:02.000Z","updated":"2019-10-25T05:58:25.240Z","comments":false,"path":"categories/index.html","permalink":"https://jetinzhang.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-10-25T05:45:52.000Z","updated":"2019-10-25T05:58:30.376Z","comments":false,"path":"tags/index.html","permalink":"https://jetinzhang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubespray自动化部署Kubernetes 1.16.0","slug":"Kubespray自动化部署Kubernetes v1.16.0","date":"2019-10-29T15:10:03.000Z","updated":"2019-10-29T15:11:04.187Z","comments":true,"path":"2019/10/29/Kubespray自动化部署Kubernetes v1.16.0/","link":"","permalink":"https://jetinzhang.github.io/2019/10/29/Kubespray%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2Kubernetes%20v1.16.0/","excerpt":"","text":"Kubespray自动化部署Kubernetes 1.16.0Author: jetinzhang &#x6a;&#101;&#116;&#105;&#110;&#x7a;&#104;&#x61;&#110;&#103;&#x40;&#49;&#x36;&#x33;&#46;&#x63;&#111;&#x6d; 1. 参考资料首先感谢以下文章的作者，才能写出这篇文档 (我不确定以下所有链接是否是原创链接) k8s 1.13.1 to kubespray 使用Kubespray自动化部署Kubernetes 1.13.1 使用Kubespray 部署kubernetes 高可用集群 使用Kubespray 2.8.3部署生产可用的Kubernetes集群(1.12.5) kubespray安装过程各节点初始化脚本 k8s镜像被墙解决方案gcr.azk8s.cn – 感谢anjia0532，在他的github中看到了README文件 感谢微软代理的kubernetes镜像,让国内通过将gcr.io替换为gcr.azk8s.cn即可直接拉取到镜像 查看k8s官方最新镜像-需要科学上网 证书轮换 2. 安装需具备的知识Linux, Ansible, Docker 3. 环境说明Ansible: v2.7.8kubespray: v2.11.0kubernetes: v1.16.0 主机名 ip地址 配置 角色 系统 docker版本 ops-01 10.1.20.7 CPU: 4核 + 内存: 8G + 硬盘: 200G Kubespray_admin debian10 (内核：4.19.37-5) 19.03.2 master1 10.1.20.81 CPU: 4核 + 内存: 8G + 硬盘: 200G master (etcd) debian10 (内核：4.19.37-5) 19.03.2 master2 10.1.20.82 CPU: 4核 + 内存: 8G + 硬盘: 200G master (etcd) debian10 (内核：4.19.37-5) 19.03.2 master3 10.1.20.83 CPU: 4核 + 内存: 8G + 硬盘: 200G master (etcd) debian10 (内核：4.19.37-5) 19.03.2 node1 10.1.20.84 CPU: 4核 + 内存: 64G + 硬盘: 500G node debian10 (内核：4.19.37-5) 19.03.2 node1 10.1.20.85 CPU: 4核 + 内存: 64G + 硬盘: 500G node debian10 (内核：4.19.37-5) 19.03.2 node1 10.1.20.86 CPU: 4核 + 内存: 64G + 硬盘: 500G node debian10 (内核：4.19.37-5) 19.03.2 3.1. docker版本要求kubernetes变更日志 The list of validated docker versions remains unchanged.The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09. (#72823, #72831) 4. 初始化环境==注意：所有k8s集群服务器上执行–除了ops-01这一台Kubespray_admin服务器== 4.1. 关闭防火墙12345678910# debian10 默认未安装selinux, 安装了的请自行关闭systemctl stop firewalldsystemctl disable firewalld# 临时关闭selinuxsetenforce 0# 永久关闭selinux (重启生效) /etc/sysconfig/selinux 是一个软链接文件所以要加上 --follow-symlinkssed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux 4.2. 关闭虚拟内存1swapoff -a 修改/etc/fstab 注释掉有swap的那一行 4.3. 设置debian iptables模式链接地址：https://wiki.debian.org/nftables https://www.debian.org/releases/stable/amd64/release-notes/ch-whats-new.zh-cn.html 网络过滤现在默认基于 nftables 框架从 iptables v1.8.2 开始，该二进制包包含了 iptables-nft 和 iptables-legacy，它们是 iptables 命令行界面的两个变种。 Buster 默认使用基于 nftables 的变种，它使用 nf_tables Linux 内核子系统。旧式（legacy）变种使用 x_tables Linux 内核子系统。可以使用 update-alternatives 系统来选择变种。 这将应用于所有相关的程序和实用工具： iptables iptables-save iptables-restore ip6tables ip6tables-save ip6tables-restore arptables arptables-save arptables-restore ebtables ebtables-save ebtables-restore 以上所有的命令都新增了 -nft 和 -legacy 变种。-nft 变种是给不能或不愿意迁移至原生的 nftables 命令行界面的用户准备的。然而，我们强烈建议您切换至 nftables 界面，而不是继续使用 iptables。 nftables 提供了 iptables 的完整替代，它具有更好的性能，全新的语法，对 IPv4/IPv6 双栈防火墙的更好的支持，用于动态更新规则集的完整的原子操作，提供给第三方应用的 Netlink API，利用改进的通用 set 和 map 基础架构实现的更快速的包分类，以及许多其他改进。 这个变化与其他主要的 Linux 发行版一致，例如 RedHat 现在也使用 nftables 作为默认的防火墙工具。 另外，请注意所有的 iptables 二进制程序现在都被安装在 /usr/sbin 而不是 /sbin。我们为了兼容性保留了一个符号链接，但它会在 buster 发布周期之后被移除。在脚本中使用的绝对路径需要被更正，并且需要避免使用。 详细的文档可以在该软件包的 README 和 NEWS 文件，以及 Debian 维基中找到。 12345678910111213# 查看现在iptable版本及模式root@kfpt-c1-master1:~# iptables -Viptables v1.8.2 (nf_tables)# iptables设置为legacy模式update-alternatives --set iptables /usr/sbin/iptables-legacy# 再次查看iptable版本及模式root@kfpt-c1-master1:~# iptables -Viptables v1.8.2 (legacy)# 还原dbian10默认iptables模式update-alternatives --set iptables /usr/sbin/iptables-nft 5. 使用kubespray一键部署kubernetes5.1. 获取kubespray并进行配置5.1.1. 安装kubespray12345678# 安装ansible密码登录方式支持 sshpassapt-get install sshpassgit clone https://github.com/kubernetes-sigs/kubespray.git# 安装 kubespray 依赖，若无特殊说明，后续操作均在~/kubespray目录下执行cd kubespraypip install -r requirements.txt 5.1.2. 复制一份配置文件12# 复制一份集群配置文件cp inventory/sample/ inventory/kfpt-cluster/ -ra 5.1.3. 修改配置文件5.1.3.1. 修改服务器配置文件inventory/kfpt-cluster/inventory.ini vim inventory/kfpt-cluster/inventory.ini 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# ## Configure 'ip' variable to bind kubernetes services on a# ## different ip than the default iface# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.[all]# node1 ansible_host=95.54.0.12 # ip=10.3.0.1 etcd_member_name=etcd1# node2 ansible_host=95.54.0.13 # ip=10.3.0.2 etcd_member_name=etcd2# node3 ansible_host=95.54.0.14 # ip=10.3.0.3 etcd_member_name=etcd3# node4 ansible_host=95.54.0.15 # ip=10.3.0.4 etcd_member_name=etcd4# node5 ansible_host=95.54.0.16 # ip=10.3.0.5 etcd_member_name=etcd5# node6 ansible_host=95.54.0.17 # ip=10.3.0.6 etcd_member_name=etcd6master1 ansible_ssh_host=10.1.20.81 ansible_ssh_user=root ansible_ssh_port=22 ansible_ssh_pass=yourpassword etcd_member_name=etcd1master2 ansible_ssh_host=10.1.20.82 ansible_ssh_user=root ansible_ssh_port=22 ansible_ssh_pass=yourpassword etcd_member_name=etcd2master3 ansible_ssh_host=10.1.20.83 ansible_ssh_user=root ansible_ssh_port=22 ansible_ssh_pass=yourpassword etcd_member_name=etcd3node1 ansible_ssh_host=10.1.20.84 ansible_ssh_user=root ansible_ssh_port=22 ansible_ssh_pass=yourpasswordnode2 ansible_ssh_host=10.1.20.85 ansible_ssh_user=root ansible_ssh_port=22 ansible_ssh_pass=yourpasswordnode3 ansible_ssh_host=10.1.20.86 ansible_ssh_user=root ansible_ssh_port=22 ansible_ssh_pass=yourpassword# ## configure a bastion host if your nodes are not directly reachable# bastion ansible_host=x.x.x.x ansible_user=some_user[kube-master]# node1# node2master1master2master3[etcd]# node1# node2# node3master1master2master3[kube-node]# node2# node3# node4# node5# node6node1node2node3[calico-rr][k8s-cluster:children]kube-masterkube-nodecalico-rr 5.1.3.2. 修改全局配置文件inventory/kfpt-cluster/group_vars/all/all.yml 12# 修改配置文件all.yamlvim inventory/kfpt-cluster/group_vars/all/all.yml 修改如下配置: 1234# 启用本地负载均衡 apiserverloadbalancer_apiserver_localhost: true# 加载内核模块，否则 ceph, gfs 等无法挂载客户端kubelet_load_modules: true 5.1.3.3. 修改下载配置文件roles/download/defaults/main.yml 一般修改这个配置即可 12# gcr_image_repo: \"gcr.io\"gcr_image_repo: \"gcr.azk8s.cn\" 或者直接替换字符串 12345678910111213141516# quay.io的镜像国内是可访问的，所以不需要替换，如果有自建docker私库的话，可以拉取下来推到私库，然后替换quay.io为私库地址，这样拉取镜像快一些。# 替换官方镜像gcr.io为微软中国私库地址gcr.azk8s.cnsed -i 's#gcr.io#gcr.azk8s.cn#g' roles/download/defaults/main.yml# 可能还涉及到需要修改的地方,此次部署暂未修改# sed -i 's#k8s.gcr.io#gcr.azk8s.cn#g' roles/download/defaults/main.yml# sed -i 's#google_containers#google-containers#g' roles/download/defaults/main.yml# 如果自建私库的话 (此次部署暂未修改)sed -i 's#gcr.io#registry-vpc.cn-shenzhen.aliyuncs.com#g' roles/download/defaults/main.yml# 可能还涉及到需要修改的地方,此次部署暂未修改# sed -i 's#k8s.gcr.io#registry-vpc.cn-shenzhen.aliyuncs.com#g' roles/download/defaults/main.yml# sed -i 's#google_containers#google-containers#g' roles/download/defaults/main.yml 5.1.3.4. 修改k8s集群部署配置文件inventory/kfpt-cluster/group_vars/k8s-cluster/k8s-cluster.yml 123456789101112131415161718# kubernetes image repo define#kube_image_repo: \"gcr.io/google-containers\"kube_image_repo: \"gcr.azk8s.cn/google-containers\"# 操作系统守护进程预留资源 (根据节点配置进行调整)## Optionally reserve resources for OS system daemons.# system_reserved: true## Uncomment to override default values# system_memory_reserved: 512M# system_cpu_reserved: 500m## Reservation for master hosts# system_master_memory_reserved: 256M# system_master_cpu_reserved: 250msystem_reserved: truesystem_memory_reserved: 2048Msystem_cpu_reserved: 1000msystem_master_memory_reserved: 1024Msystem_master_cpu_reserved: 500m 5.1.3.5. 修改k8s node节点kubelet相关配置vim roles/kubernetes/node/defaults/main.yml 根据自身服务器配置调整相关参数 (根据节点cpu、内存配置进行适当的调整) ，此次部署调整了以下参数 kubelet_custom_flags 和 kubelet_node_custom_flags参数比较关键，用于内存不够时驱赶pod==kubelet_custom_flags (设置master节点)kubelet_node_custom_flags (设置node节点)== 123456# 设置硬性内存驱赶条件 (个人理解)- \"--eviction-hard=memory.available&lt;800Mi\"# 软驱赶时间间隔- \"--eviction-soft-grace-period=memory.available=30s\"# 设置软性内存驱赶条件 - 软性驱动条件应该大于硬性驱动条件 (个人理解)- \"--eviction-soft=memory.available&lt;1200Mi\" ==如果未配置这两个选项 node节点可能会因内存不足导致节点挂掉，导致大量应用无法正常使用，出现以下状况。== 12345678root@kfpt-c1-master1:~# kubectl get nodesNAME STATUS ROLES AGE VERSIONkfpt-c1-master1 Ready master 30h v1.16.0kfpt-c1-master2 Ready master 30h v1.16.0kfpt-c1-master3 Ready master 30h v1.16.0kfpt-c1-node1 NotReady &lt;none&gt; 30h v1.16.0kfpt-c1-node2 NotReady &lt;none&gt; 30h v1.16.0kfpt-c1-node3 NotReady &lt;none&gt; 30h v1.16.0 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Reserve this space for kube resources# kube_memory_reserved: 256M# kube_cpu_reserved: 100mkube_memory_reserved: 2048Mkube_cpu_reserved: 1200m# Reservation for master hosts# kube_master_memory_reserved: 512M# kube_master_cpu_reserved: 200mkube_master_memory_reserved: 1500Mkube_master_cpu_reserved: 800mkubelet_status_update_frequency: 10s# Requests for load balancer app# loadbalancer_apiserver_memory_requests: 32M# loadbalancer_apiserver_cpu_requests: 25mloadbalancer_apiserver_memory_requests: 100Mloadbalancer_apiserver_cpu_requests: 50m# Uncomment if you need to enable deprecated runtimeskube_api_runtime_config:# - apps/v1beta1=true# - apps/ve1beta2=true - extensions/v1beta1/daemonsets=true - extensions/v1beta1/deployments=true# - extensions/v1beta1/replicasets=true# - extensions/v1beta1/networkpolicies=true# - extensions/v1beta1/podsecuritypolicies=true# A port range to reserve for services with NodePort visibility.# Inclusive at both ends of the range.kube_apiserver_node_port_range: \"30000-32767\"# Configure the amount of pods able to run on single node# default is equal to application defaultkubelet_max_pods: 110## Support custom flags to be passed to kubelet#kubelet_custom_flags: []kubelet_custom_flags: - \"--eviction-hard=memory.available&lt;800Mi\" - \"--eviction-soft-grace-period=memory.available=30s\" - \"--eviction-soft=memory.available&lt;1200Mi\"## Support custom flags to be passed to kubelet only on nodes, not masters#kubelet_node_custom_flags: []kubelet_node_custom_flags: - \"--eviction-hard=memory.available&lt;2000Mi\" - \"--eviction-soft-grace-period=memory.available=30s\" - \"--eviction-soft=memory.available&lt;5000Mi\" 5.1.3.6. 修改扩展组件配置文件inventory/kfpt-cluster/group_vars/k8s-cluster/addons.yml 启用top支持，安装metrics_server。 开启证书轮换 123456# metrics_server_enabled: falsemetrics_server_enabled: true# registry_disk_size: \"10Gi\"utomatically generate a new key and request a new certificate from the Kubernetes API as the current certificate approaches expiration# kubelet_rotate_certificates: truekubelet_rotate_certificates: true 若觉得metrics_server版本不够新可更改一下版本号vi roles/download/defaults/main.yml 12# metrics_server_version: \"v0.3.3\"metrics_server_version: \"v0.3.5\" 启用Dashboard 和 helm 1234567# Kubernetes dashboard# RBAC required. see docs/getting-started.md for access details.dashboard_enabled: true# Helm deployment# helm_enabled: falsehelm_enabled: true 5.1.3.7. 修改k8s dashboard 模板文件roles/kubernetes-apps/ansible/templates/dashboard.yml.j2 修改代码，使用NodePort方式访问Dashboard。 vim roles/kubernetes-apps/ansible/templates/dashboard.yml.j2 123456789101112# ------------------- Dashboard Service ------------------- #……前面部分省略……spec: type: NodePort # 添加这一行 ports: - port: 443 targetPort: 8443 nodePort: 30001 # 添加这一行 5.1.3.8. 修改docker参数inventory/kfpt-cluster/group_vars/all/docker.yml vim inventory/kfpt-cluster/group_vars/all/docker.yml 其他参数根据自身需要进行调整 1234567## Used to set docker daemon iptables options to truedocker_iptables_enabled: \"false\"# Docker log options# Rotate container stderr/stdout logs at 50m and keep last 5docker_log_opts: \"--log-opt max-size=100m --log-opt max-file=5\" 5.1.3.9. k8s虚拟内存检查是否启用（建议禁用虚拟内存）roles/kubernetes/node/defaults/main.yml 12# 禁用虚拟内存检查vim roles/kubernetes/node/defaults/main.yml 123### 5.2. fail with swap on (default true)kubelet_fail_swap_on: true# kubelet_fail_swap_on: false 5.2. 开始安装kubernetes1ansible-playbook -i inventory/kfpt-cluster/inventory.ini cluster.yml -b -v 安装完成 5.2.1. 查看k8s集群状态12345678root@kfpt-c1-master1:~# kubectl get nodes -owideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEkfpt-c1-master1 Ready master 135m v1.16.0 10.1.20.81 &lt;none&gt; Debian GNU/Linux 10 (buster) 4.19.0-5-amd64 docker://18.9.7kfpt-c1-master2 Ready master 133m v1.16.0 10.1.20.82 &lt;none&gt; Debian GNU/Linux 10 (buster) 4.19.0-5-amd64 docker://18.9.7kfpt-c1-master3 Ready master 133m v1.16.0 10.1.20.83 &lt;none&gt; Debian GNU/Linux 10 (buster) 4.19.0-5-amd64 docker://18.9.7kfpt-c1-node1 Ready &lt;none&gt; 131m v1.16.0 10.1.20.84 &lt;none&gt; Debian GNU/Linux 10 (buster) 4.19.0-5-amd64 docker://18.9.7kfpt-c1-node2 Ready &lt;none&gt; 131m v1.16.0 10.1.20.85 &lt;none&gt; Debian GNU/Linux 10 (buster) 4.19.0-5-amd64 docker://18.9.7kfpt-c1-node3 Ready &lt;none&gt; 131m v1.16.0 10.1.20.86 &lt;none&gt; Debian GNU/Linux 10 (buster) 4.19.0-5-amd64 docker://18.9.7 5.2.2. 测试应用部署1kubectl create deployment nginx --image=nginx dashboard https证书访问12345678910111213# 删除自动创建的证书kubectl delete secret kubernetes-dashboard-certs -n kube-system# 通过自己的证书, 挂载给kubernetes-dashboard使用，创建方法有两种# 创建方法一kubectl create secret generic kubernetes-dashboard-certs --from-file=certs/ -n kube-system# 创建方法二kubectl create secret generic kubernetes-dashboard-certs --from-file=certs/dashboard.key --from-file=certs/dashboard.crt -n kube-system Dashboard yaml文件示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------- Dashboard Secret ------------------- #apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret.- apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map.- apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"create\"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.- apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics from heapster.- apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\"] verbs: [\"proxy\"]- apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"] verbs: [\"get\"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects:- kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- ## apiVersion: extensions/v1beta1apiVersion: apps/v1kind: Deploymentmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: kubernetes-dashboard spec: containers: - args:# - --auto-generate-certificates# 通过自己的证书来访问k8s dashboard 要注释掉 --auto-generate-certificates 并增加 --auto-generate-certificates --tls-cert-file=dashboard.pem - --tls-key-file=dashboard.key - --tls-cert-file=dashboard.pem - --authentication-mode=token - --token-ttl=900 image: gcr.azk8s.cn/google-containers/kubernetes-dashboard-amd64:v1.10.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: / port: 8443 scheme: HTTPS initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 30 name: kubernetes-dashboard ports: - containerPort: 8443 protocol: TCP resources: limits: cpu: 100m memory: 256M requests: cpu: 50m memory: 64M terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /certs name: kubernetes-dashboard-certs - mountPath: /tmp name: tmp-volume dnsPolicy: ClusterFirst priorityClassName: system-cluster-critical restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: kubernetes-dashboard serviceAccountName: kubernetes-dashboard terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - name: kubernetes-dashboard-certs secret: defaultMode: 420 secretName: kubernetes-dashboard-certs - emptyDir: &#123;&#125; name: tmp-volume---apiVersion: v1kind: Servicemetadata: labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" name: kubernetes-dashboard namespace: kube-systemspec: ports: - port: 443 protocol: TCP targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard type: NodePort 使用 kubeconfig 或 token 进行用户身份认证参考文档-1 参考文档-2 12# 查看证书kubectl get secret --namespace kube-system 在原始的yaml文件中增加这一部分 1234567891011121314151617181920212223242526272829# 比原版增加了admin账号绑定 修改了命名空间 ---- start---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\"roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: admin namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile# 比原版增加了admin账号绑定 ---- end 12# 获取admin的tokenkubectl describe secret/$(kubectl get secret -n kube-system |grep admin|awk '&#123;print $1&#125;') -n kube-system 其他命令 123456789101112131415161718192021# 查看rolekubectl get role -n kube-systemkubectl get rolebinding -n kube-systemkubectl get clusterrolekubectl get clusterrolebinding# 查看登录dashboard tokenkubectl describe secret -n kube-system kubernetes-dashboard-token# 或者 获取admin的tokenkubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token # 查看kubectl get clusterrolebinding/login-on-dashboard-with-cluster-admin -o yaml# 创建Dashboardkubectl apply -f kubernetes-dashboard.yaml# 删除Dashboardkubectl delete -f kubernetes-dashboard.yaml 5.2.3. 配置docker其他参数==docker其他参数此次部署未做调整== 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 如果启用了用docker iptables 可以调整以下参数保证网络可访问docker_iptables_enabled: \"false\"# docker_iptables_enabled: \"true\"# Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信，在各个Docker节点执行下面的命令：iptables -P FORWARD ACCEPT# 编辑systemctl的Docker启动文件sed -i \"13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT\" /usr/lib/systemd/system/docker.service# 可能在14行具体根据情况调整sed -i \"14i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT\" /usr/lib/systemd/system/docker.service# 新建配置文件 (根据自身情况调整日志格式、http私库地址、存储驱动方式、存储选项等等)cat /etc/docker/daemon.json&#123;\"log-driver\": \"json-file\",\"insecure-registries\": [ \"docker.local.com # 本地私库地址无https需要将域名或者ip地址添加到此处\"],\"storage-driver\": \"overlay2\",\"storage-opts\":[\"overlay2.override_kernel_check=true\"]&#125;# 还可通过 /etc/docker/daemon.json禁止docker自动设定防火墙# 注意，可能像docker-compose需要用到iptables做为路由功能的情况，关闭的话可能会受到影响，则可采用这种方式：sed -i \"13i ExecStartPost=/usr/sbin/iptables -P FORWARD ACCEPT\" /usr/lib/systemd/system/docker.servicevi /etc/docker/daemon.json&#123; \"iptables\": false&#125;# kubespray 部署会自动配置日志大小，如果你配置了\"log-opts\"部分就要去掉, 不然无法启动docker会报错 (出现两个参数冲突)kubespray安装docker的配置文件目录/etc/systemd/system/docker.service.droot@kfpt-c1-master1:~# ll /etc/systemd/system/docker.service.d/total 8-rw-r--r-- 1 root root 245 Oct 8 11:07 docker-dns.conf-rw-r--r-- 1 root root 215 Oct 8 11:07 docker-options.conf===如果配置了 /etc/docker/daemon.json 与其参数有冲突则会报错===\"unable to configure the Docker daemon with file /etc/docker/daemon.json: the following directives are specified both as a flag\" \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"5\" &#125;, 6. 证书过期时间检查检查证书过期时间 12345# For kubeadm provisioned clusterskubeadm --config /etc/kubernetes/kubeadm-config.yaml alpha certs check-expiration# For all clustersopenssl x509 -noout -dates -in /etc/kubernetes/pki/apiserver-kubelet-client.crt 证书轮换请查看链接 7. kubespray使用命令说明1234567891011121314151617181920# 集群部署或更新配置ansible-playbook -i inventory/kfpt-cluster/inventory.ini cluster.yml -b -v# 如果需要扩容node节点，则修改inventory.ini 文件，增加新增的机器信息 (-k, --ask-pass 询问密码, -b 提升权限)。然后执行下面的命令ansible-playbook -i inventory/kfpt-cluster/inventory.ini scale.yml -b -v -k# 将inventory.ini文件中的master和etcd的机器增加到多台，执行部署命令ansible-playbook -i inventory/kfpt-cluster/inventory.ini cluster.yml -b -vvv# 刪除节点，如果不指定节点就是刪除整个集群ansible-playbook -i inventory/kfpt-cluster/inventory.ini remove-node.yml -b -v# 如果需要卸载，可以执行以下命令ansible-playbook -i inventory/kfpt-cluster/inventory.ini reset.yml -b –vvv# 升级K8s集群，选择对应的k8s版本信息，执行升级命令。涉及文件为upgrade-cluster.ymlansible-playbook upgrade-cluster.yml -b -i inventory/kfpt-cluster/inventory.ini -e kube_version=vX.XX.XX -vvv# 查看ipvsipvsadm -L -n 8. kubectl top支持默认安装好的k8s集群不支持kubectl top查看占用资源 ==推荐方案一：原因metrics-server长期更新，heapster + influxdb k8s已不再维护== 方案一：metrics-server因k8s从 v1.8 开始，资源使用情况的度量（如容器的 CPU 和内存使用）可以通过 Metrics API 获取。注意 Metrics API 只可以查询当前的度量数据，并不保存历史数据 Metrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 维护 必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据 安装注意：可通过github上的yaml文件进行安装需要修改镜像和增加启动命令 123456789101112$ cd deploy/1.8+/$ diff metrics-server-deployment.yaml metrics-server-deployment.yaml.default32,33c32&lt; #image: k8s.gcr.io/metrics-server-amd64:v0.3.5&lt; image: gcr.azk8s.cn/google-containers/metrics-server-amd64:v0.3.5---&gt; image: k8s.gcr.io/metrics-server-amd64:v0.3.535,38d33&lt; command:&lt; - /metrics-server&lt; - --kubelet-insecure-tls&lt; - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname 方案二：heapster + influxdb 附 heapster + influxdb yaml文件 展开查看 heapster.yaml 文件 apiVersion: v1 kind: ServiceAccount metadata: name: heapster namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: heapster namespace: kube-system spec: replicas: 1 selector: matchLabels: k8s-app: heapster template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster #image: k8s.gcr.io/google-containers/heapster-amd64:v1.5.4 image: gcr.azk8s.cn/google-containers/heapster-amd64:v1.5.4 # image: registry-vpc.cn-shenzhen.aliyuncs.com/google-containers/heapster-amd64:v1.5.4 imagePullPolicy: IfNotPresent command: - \"/heapster\" # 配置说明请查阅https://github.com/kubernetes-retired/heapster/blob/master/docs/source-configuration.md - \"--source=kubernetes:https://kubernetes.default?kubeletPort=10250&kubeletHttps=true&insecure=true\" - \"--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086\" apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: ‘true’ kubernetes.io/name: Heapster name: heapster namespace: kube-systemspec: ports: port: 80targetPort: 8082selector:k8s-app: heapster apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: heapster namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: heapstersubjects: kind: ServiceAccountname: heapsternamespace: kube-system kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapstersubjects: kind: ServiceAccountname: heapsternamespace: kube-systemroleRef:kind: ClusterRolename: cluster-adminapiGroup: rbac.authorization.k8s.io 展开查看 influxdb.yaml 文件 apiVersion: apps/v1 kind: Deployment metadata: name: monitoring-influxdb namespace: kube-system spec: replicas: 1 selector: matchLabels: k8s-app: influxdb template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb #image: k8s.gcr.io/google-containers/heapster-influxdb-amd64:v1.5.2 image: gcr.azk8s.cn/google-containers/heapster-influxdb-amd64:v1.5.2 # image: registry-vpc.cn-shenzhen.aliyuncs.com/google-containers/heapster-influxdb-amd64:v1.5.2 volumeMounts: - mountPath: /data name: influxdb-storage volumes: - name: influxdb-storage emptyDir: {} --- apiVersion: v1 kind: Service metadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-influxdb name: monitoring-influxdb namespace: kube-system spec: ports: - port: 8086 targetPort: 8086 selector: k8s-app: influxdb 9. 对kubespray文件修改了以下文件9.1. 修改文件的列表如下根据自己的需求进行调整以下文件： roles/download/defaults/main.ymlroles/kubernetes-apps/ansible/templates/dashboard.yml.j2roles/kubernetes/node/defaults/main.yml 9.2. 新增的配置文件夹inventory/kfpt-cluster 新增文件夹后根据自己的需求进行调整以下文件： inventory/kfpt-cluster/inventory.iniinventory/kfpt-cluster/group_vars/all/docker.ymlinventory/kfpt-cluster/group_vars/k8s-cluster/k8s-cluster.ymlinventory/kfpt-cluster/group_vars/k8s-cluster/addons.yml 10. kubespray部署高可用原理所有node节点都部署了nginx，在各自node节点上代理了三个master节点api地址。然后kubelet 直接访问节点本机的代理地址实现高可用。 11. 遇到错误问题 debian10部署时遇到问题 本机负载均衡apiserver地址后master节点有时候通有时候不能通，node节点完全无法请求的问题，master和node都可以ping通，但请求就不一定通。 curl -k https://10.253.0.1 三次只有一次请求不到。 123456ipvsadm -L -nTCP 10.253.0.1:443 rr -&gt; 10.1.20.81:6443 Masq 1 0 0 -&gt; 10.1.20.82:6443 Masq 1 0 0 -&gt; 10.1.20.83:6443 Masq 1 1 0 在查找原因过程中，发现组件kube-proxy也报错了 E1008 02:53:00.722967 1 proxier.go:1233] Failed to execute iptables-restore: exit status 2 (iptables-restore v1.6.0: Couldn’t load target `KUBE-MARK-DROP’:No such file or directory Error occurred at line: 15Try `iptables-restore -h’ or ‘iptables-restore –help’ for more information. ==原因：可能是iptables模式未设置成 legacy 模式== 12. 其他 K8s新版本中，针对无状态类服务推荐使用Deployment，有状态类服务则建议使用Statefulset。RC和RS已不支持目前K8s的诸多新特性了。 K8s从1.11版本起便废弃了Heapster监控组件，取而代之的是metrics-server 和 custom metrics API，后面将陆续完善包括Prometheus+Grafana监控，Kibana+Fluentd日志管理，cephfs-provisioner存储（可能需要重新build kube-controller-manager装上rbd相关的包才能使用Ceph RBD StorageClass），traefik ingress等服务。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jetinzhang.github.io/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://jetinzhang.github.io/tags/kubernetes/"},{"name":"kubespray","slug":"kubespray","permalink":"https://jetinzhang.github.io/tags/kubespray/"},{"name":"docker","slug":"docker","permalink":"https://jetinzhang.github.io/tags/docker/"},{"name":"ansible","slug":"ansible","permalink":"https://jetinzhang.github.io/tags/ansible/"}]},{"title":"Linux 分区MBR调整为GPT，并扩大容量","slug":"Linux 分区MBR调整为GPT并扩大容量","date":"2019-10-25T11:01:03.000Z","updated":"2019-10-29T14:19:03.146Z","comments":true,"path":"2019/10/25/Linux 分区MBR调整为GPT并扩大容量/","link":"","permalink":"https://jetinzhang.github.io/2019/10/25/Linux%20%E5%88%86%E5%8C%BAMBR%E8%B0%83%E6%95%B4%E4%B8%BAGPT%E5%B9%B6%E6%89%A9%E5%A4%A7%E5%AE%B9%E9%87%8F/","excerpt":"","text":"Linux 分区MBR调整为GPT，并扩大容量==注意: 涉及到重要数据，请先备份！！！！！！== 1. 说明具备知识：fdisk gdisk基础使用方法 使用工具：gdisk ==注意: 不要使用fdisk调整GPT分区，也不要使用gdisk去调整MBR分区，但转换除外。== 之前看有些文章使用parted来调整分区也是可以的，但不建议，因为parted执行完命令是立即生效的，建议还是使用gdisk 类似fdisk操作，不保存不生效。 2. 安装工具12345# Debianapt-get install gdisk# Centosyum install gdisk 3. 卸载硬盘挂载的目录1umount /data 4. 判断硬盘类型使用如下命令： 1234567891011121314151617181920root@minio-c1-node2:~# fdisk -l /dev/sdbDisk /dev/sdb: 5 TiB, 5497558138880 bytes, 10737418240 sectorsDisk model: Virtual disk Units: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: BEAA23E5-F7C0-4CB4-9A45-5666024DDAE3Device Start End Sectors Size Type/dev/sdb1 2048 10737418206 10737416159 5T Linux filesystem-----------------# GPT类型Disklabel type: gpt # MBR类型Disklabel type: dos 5. MBR转换为GPT并扩容如果只是想转换为gpt的话 gdisk /dev/sdb 然后在gdisk命令行输入w 保存一下就实现了mbr转换为gpt了。 如果还要继续扩大容量使用gdisk删除原来分区，重新建分区也能进行转换并扩大容量。 ==注意：原来Start (sector) 是多少，First sector就填多少，这样才能保证文件不丢失== 5.1. 转换为gpt12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061root@minio-c1-node4:~# gdisk /dev/sdbGPT fdisk (gdisk) version 1.0.3Partition table scan: MBR: protective BSD: not present APM: not present GPT: presentFound valid GPT with protective MBR; using GPT.Command (? for help): pDisk /dev/sdb: 10737418240 sectors, 5.0 TiBModel: Virtual disk Sector size (logical/physical): 512/512 bytesDisk identifier (GUID): 6BC2B78A-FD8D-4103-803C-2628F7C4C765Partition table holds up to 128 entriesMain partition table begins at sector 2 and ends at sector 33First usable sector is 34, last usable sector is 10737418206Partitions will be aligned on 2048-sector boundariesTotal free space is 8589936573 sectors (4.0 TiB)Number Start (sector) End (sector) Size Code Name 1 2048 2147483647 1024.0 GiB 8300 Linux filesystemCommand (? for help): dUsing 1Command (? for help): nPartition number (1-128, default 1): First sector (34-10737418206, default = 2048) or &#123;+-&#125;size&#123;KMGTP&#125;: Last sector (2048-10737418206, default = 10737418206) or &#123;+-&#125;size&#123;KMGTP&#125;: Current type is 'Linux filesystem'Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to 'Linux filesystem'Command (? for help): pDisk /dev/sdb: 10737418240 sectors, 5.0 TiBModel: Virtual disk Sector size (logical/physical): 512/512 bytesDisk identifier (GUID): 6BC2B78A-FD8D-4103-803C-2628F7C4C765Partition table holds up to 128 entriesMain partition table begins at sector 2 and ends at sector 33First usable sector is 34, last usable sector is 10737418206Partitions will be aligned on 2048-sector boundariesTotal free space is 2014 sectors (1007.0 KiB)Number Start (sector) End (sector) Size Code Name 1 2048 10737418206 5.0 TiB 8300 Linux filesystemCommand (? for help): wFinal checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTINGPARTITIONS!!Do you want to proceed? (Y/N): YOK; writing new GUID partition table (GPT) to /dev/sdb.Warning: The kernel is still using the old partition table.The new table will be used at the next reboot or after yourun partprobe(8) or kpartx(8)The operation has completed successfully. 5.2. 调整分区大小123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657root@minio-c1-node3:~# gdisk /dev/sdbGPT fdisk (gdisk) version 1.0.3Partition table scan: MBR: MBR only BSD: not present APM: not present GPT: not present***************************************************************Found invalid GPT and valid MBR; converting MBR to GPT formatin memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit bytyping 'q' if you don't want to convert your MBR partitionsto GPT format!***************************************************************Command (? for help): pDisk /dev/sdb: 10737418240 sectors, 5.0 TiBModel: Virtual disk Sector size (logical/physical): 512/512 bytesDisk identifier (GUID): 17A7407E-8FD7-474F-84F4-5278551BBAE7Partition table holds up to 128 entriesMain partition table begins at sector 2 and ends at sector 33First usable sector is 34, last usable sector is 10737418206Partitions will be aligned on 2048-sector boundariesTotal free space is 8589936573 sectors (4.0 TiB)Number Start (sector) End (sector) Size Code Name 1 2048 2147483647 1024.0 GiB 8300 Linux filesystemCommand (? for help): dUsing 1Command (? for help): nPartition number (1-128, default 1): First sector (34-10737418206, default = 2048) or &#123;+-&#125;size&#123;KMGTP&#125;: Last sector (2048-10737418206, default = 10737418206) or &#123;+-&#125;size&#123;KMGTP&#125;: Current type is 'Linux filesystem'Hex code or GUID (L to show codes, Enter = 8300): Changed type of partition to 'Linux filesystem'Command (? for help): pDisk /dev/sdb: 10737418240 sectors, 5.0 TiBModel: Virtual disk Sector size (logical/physical): 512/512 bytesDisk identifier (GUID): CC9ED012-8B6C-4682-B9FF-65A4CD1E92A0Partition table holds up to 128 entriesMain partition table begins at sector 2 and ends at sector 33First usable sector is 34, last usable sector is 10737418206Partitions will be aligned on 2048-sector boundariesTotal free space is 2014 sectors (1007.0 KiB)Number Start (sector) End (sector) Size Code Name 1 2048 10737418206 5.0 TiB 8300 Linux filesystem 6. 通知系统硬盘大小已调整只有进行下面这一步骤，df -hT时才会显示正常的硬盘空间大小 1234567891011121314151617181920# 首先查看是否调整完分区后自动挂载分区了,如果挂载了要再卸载一次。(我这边是会出现自动挂载现象，如果没有卸载会出现问题1的错误。)root@minio-c1-node4:~# df -hTFilesystem Type Size Used Avail Use% Mounted onudev devtmpfs 3.9G 0 3.9G 0% /devtmpfs tmpfs 798M 8.6M 790M 2% /run/dev/sda2 ext4 58G 2.3G 53G 5% /tmpfs tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs tmpfs 5.0M 0 5.0M 0% /run/locktmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 ext4 1000M 47M 885M 5% /boottmpfs tmpfs 798M 0 798M 0% /run/user/0/dev/sdb1 ext4 1007G 158M 956G 1% /data# ext分区格使用命令进行调整resize2fs /dev/sdb1# xfs分区格式使用命令进行调整xfs_growfs /dev/sdb1 7. 遇到问题问题1: The filesystem is already 268435200 (4k) blocks long. Nothing to do! 将挂载的分区再次卸载，然后再执行resize2fs或者xfs_growfs命令。 12345678910111213141516root@minio-c1-node4:~# resize2fs /dev/sdb1 resize2fs 1.44.5 (15-Dec-2018)The filesystem is already 268435200 (4k) blocks long. Nothing to do!root@minio-c1-node4:~# df -hTFilesystem Type Size Used Avail Use% Mounted onudev devtmpfs 3.9G 0 3.9G 0% /devtmpfs tmpfs 798M 8.6M 790M 2% /run/dev/sda2 ext4 58G 2.3G 53G 5% /tmpfs tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs tmpfs 5.0M 0 5.0M 0% /run/locktmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/sda1 ext4 1000M 47M 885M 5% /boottmpfs tmpfs 798M 0 798M 0% /run/user/0/dev/sdb1 ext4 1007G 158M 956G 1% /data","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jetinzhang.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jetinzhang.github.io/tags/Linux/"},{"name":"gdisk","slug":"gdisk","permalink":"https://jetinzhang.github.io/tags/gdisk/"},{"name":"分区","slug":"分区","permalink":"https://jetinzhang.github.io/tags/%E5%88%86%E5%8C%BA/"},{"name":"扩容","slug":"扩容","permalink":"https://jetinzhang.github.io/tags/%E6%89%A9%E5%AE%B9/"}]}]}